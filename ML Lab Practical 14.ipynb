{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5iB5X00D8a"
      },
      "source": [
        "Lab 14: Model Evaluation and Tuning\n",
        "This script demonstrates model evaluation metrics and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xpaQRIH-0D8c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer, load_iris\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
        "                                     GridSearchCV, RandomizedSearchCV, learning_curve)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve,\n",
        "                            classification_report, confusion_matrix)\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XLhAM_Yq0D8d"
      },
      "outputs": [],
      "source": [
        "def evaluate_classification_metrics():\n",
        "    \"\"\"Demonstrate various classification metrics\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Classification Metrics\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    cancer = load_breast_cancer()\n",
        "    X = cancer.data\n",
        "    y = cancer.target\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=cancer.target_names,\n",
        "                yticklabels=cancer.target_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_confusion_matrix.png')\n",
        "    plt.close()\n",
        "    print(\"\\nConfusion matrix saved as 'lab14_confusion_matrix.png'\")\n",
        "\n",
        "    return y_test, y_pred_proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bW-qCyLE0D8e"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curves(y_test, y_pred_proba):\n",
        "    \"\"\"Plot ROC curves\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ROC Curve Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"\\nArea Under ROC Curve: {roc_auc:.4f}\")\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_roc_curve.png')\n",
        "    plt.close()\n",
        "    print(\"\\nROC curve saved as 'lab14_roc_curve.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WGNxIx3I0D8e"
      },
      "outputs": [],
      "source": [
        "def cross_validation_evaluation():\n",
        "    \"\"\"Demonstrate cross-validation\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Cross-Validation Evaluation\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Create model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "    print(\"\\n5-Fold Cross-Validation Results:\")\n",
        "    print(f\"Scores: {cv_scores}\")\n",
        "    print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "    print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "    print(f\"95% Confidence Interval: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, \"\n",
        "          f\"{cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='skyblue', edgecolor='black')\n",
        "    plt.axhline(y=cv_scores.mean(), color='r', linestyle='--',\n",
        "                label=f'Mean: {cv_scores.mean():.4f}')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Cross-Validation Scores')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_cross_validation.png')\n",
        "    plt.close()\n",
        "    print(\"\\nCross-validation plot saved as 'lab14_cross_validation.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vGlFbZ6m0D8e"
      },
      "outputs": [],
      "source": [
        "def grid_search_tuning():\n",
        "    \"\"\"Demonstrate Grid Search for hyperparameter tuning\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Grid Search Hyperparameter Tuning\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7, None],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "\n",
        "    print(\"\\nParameter Grid:\")\n",
        "    for param, values in param_grid.items():\n",
        "        print(f\"  {param}: {values}\")\n",
        "\n",
        "    # Create model\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Perform Grid Search\n",
        "    print(\"\\nPerforming Grid Search...\")\n",
        "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nBest Parameters:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    best_model = grid_search.best_estimator_\n",
        "    test_score = best_model.score(X_test, y_test)\n",
        "    print(f\"Test Set Score: {test_score:.4f}\")\n",
        "\n",
        "    # Visualize results\n",
        "    results = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "    # Plot top configurations\n",
        "    results_sorted = results.sort_values('mean_test_score', ascending=False).head(10)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x_pos = range(len(results_sorted))\n",
        "    plt.barh(x_pos, results_sorted['mean_test_score'], color='lightblue', edgecolor='black')\n",
        "    plt.yticks(x_pos, [f\"Config {i+1}\" for i in range(len(results_sorted))])\n",
        "    plt.xlabel('Mean Cross-Validation Score')\n",
        "    plt.title('Top 10 Hyperparameter Configurations')\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_grid_search.png')\n",
        "    plt.close()\n",
        "    print(\"\\nGrid search results saved as 'lab14_grid_search.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h1hwNx2w0D8f"
      },
      "outputs": [],
      "source": [
        "def random_search_tuning():\n",
        "    \"\"\"Demonstrate Random Search for hyperparameter tuning\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Random Search Hyperparameter Tuning\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    cancer = load_breast_cancer()\n",
        "    X = cancer.data\n",
        "    y = cancer.target\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Define parameter distribution\n",
        "    param_dist = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "        'kernel': ['rbf', 'poly']\n",
        "    }\n",
        "\n",
        "    print(\"\\nParameter Distribution:\")\n",
        "    for param, values in param_dist.items():\n",
        "        print(f\"  {param}: {values}\")\n",
        "\n",
        "    # Create model\n",
        "    svc = SVC(random_state=42)\n",
        "\n",
        "    # Perform Random Search\n",
        "    print(\"\\nPerforming Random Search (2 iterations)...\") # Reduced iterations\n",
        "    random_search = RandomizedSearchCV(\n",
        "        svc, param_dist, n_iter=2, cv=3, # Reduced iterations\n",
        "        scoring='accuracy', random_state=42, n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nBest Parameters:\")\n",
        "    for param, value in random_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    print(f\"\\nBest Cross-Validation Score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    best_model = random_search.best_estimator_\n",
        "    test_score = best_model.score(X_test, y_test)\n",
        "    print(f\"Test Set Score: {test_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sl44rAh_0D8g"
      },
      "outputs": [],
      "source": [
        "def learning_curve_analysis():\n",
        "    \"\"\"Analyze learning curves to detect overfitting/underfitting\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Learning Curve Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    cancer = load_breast_cancer()\n",
        "    X = cancer.data\n",
        "    y = cancer.target\n",
        "\n",
        "    # Create model\n",
        "    model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    # Calculate learning curve\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        model, X, y, cv=5, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "\n",
        "    # Calculate mean and std\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    print(\"\\nLearning Curve Summary:\")\n",
        "    print(f\"Final Training Score: {train_mean[-1]:.4f} (+/- {train_std[-1]:.4f})\")\n",
        "    print(f\"Final Validation Score: {val_mean[-1]:.4f} (+/- {val_std[-1]:.4f})\")\n",
        "\n",
        "    # Plot learning curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
        "                     alpha=0.1, color='blue')\n",
        "    plt.plot(train_sizes, val_mean, 'o-', color='green', label='Validation Score')\n",
        "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n",
        "                     alpha=0.1, color='green')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_learning_curve.png')\n",
        "    plt.close()\n",
        "    print(\"\\nLearning curve saved as 'lab14_learning_curve.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2OCD2q2i0D8g"
      },
      "outputs": [],
      "source": [
        "def compare_models():\n",
        "    \"\"\"Compare different models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Model Comparison\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "        'SVM': SVC(random_state=42)\n",
        "    }\n",
        "\n",
        "    # Evaluate each model\n",
        "    results = {}\n",
        "    print(\"\\nEvaluating models with 5-fold cross-validation:\")\n",
        "\n",
        "    for name, model in models.items():\n",
        "        scores = cross_val_score(model, X, y, cv=5)\n",
        "        results[name] = {\n",
        "            'mean': scores.mean(),\n",
        "            'std': scores.std(),\n",
        "            'scores': scores\n",
        "        }\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Mean Accuracy: {scores.mean():.4f}\")\n",
        "        print(f\"  Std Dev: {scores.std():.4f}\")\n",
        "\n",
        "    # Visualize comparison\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Bar plot\n",
        "    model_names = list(results.keys())\n",
        "    means = [results[name]['mean'] for name in model_names]\n",
        "    stds = [results[name]['std'] for name in model_names]\n",
        "\n",
        "    ax1.bar(model_names, means, yerr=stds, capsize=5,\n",
        "            color=['blue', 'green', 'orange'], alpha=0.7, edgecolor='black')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_title('Model Comparison (Mean Â± Std)')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    ax1.set_ylim([0.9, 1.0])\n",
        "\n",
        "    # Box plot\n",
        "    all_scores = [results[name]['scores'] for name in model_names]\n",
        "    ax2.boxplot(all_scores, labels=model_names)\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Model Comparison (Distribution)')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    ax2.set_ylim([0.9, 1.0])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_model_comparison.png')\n",
        "    plt.close()\n",
        "    print(\"\\nModel comparison plot saved as 'lab14_model_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aga4kKqn0D8h"
      },
      "outputs": [],
      "source": [
        "def bias_variance_tradeoff():\n",
        "    \"\"\"Demonstrate bias-variance tradeoff\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Bias-Variance Tradeoff\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    cancer = load_breast_cancer()\n",
        "    X = cancer.data\n",
        "    y = cancer.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Test different max_depth values\n",
        "    max_depths = range(1, 21)\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "\n",
        "    print(\"\\nTesting different model complexities:\")\n",
        "    for depth in max_depths:\n",
        "        model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        train_score = model.score(X_train, y_train)\n",
        "        test_score = model.score(X_test, y_test)\n",
        "\n",
        "        train_scores.append(train_score)\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "        if depth % 5 == 0:\n",
        "            print(f\"Max Depth {depth}: Train={train_score:.4f}, Test={test_score:.4f}\")\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(max_depths, train_scores, 'o-', label='Training Score')\n",
        "    plt.plot(max_depths, test_scores, 's-', label='Test Score')\n",
        "    plt.xlabel('Model Complexity (Max Depth)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Bias-Variance Tradeoff')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Mark regions\n",
        "    plt.axvspan(1, 5, alpha=0.1, color='red', label='High Bias (Underfitting)')\n",
        "    plt.axvspan(15, 20, alpha=0.1, color='blue', label='High Variance (Overfitting)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lab14_bias_variance.png')\n",
        "    plt.close()\n",
        "    print(\"\\nBias-variance plot saved as 'lab14_bias_variance.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "s1WUTkTk0D8h"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to demonstrate model evaluation and tuning\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Lab 14: Model Evaluation and Tuning\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    y_test, y_pred_proba = evaluate_classification_metrics()\n",
        "\n",
        "    # ROC curves\n",
        "    plot_roc_curves(y_test, y_pred_proba)\n",
        "\n",
        "    # Cross-validation\n",
        "    cross_validation_evaluation()\n",
        "\n",
        "    # Grid search\n",
        "    grid_search_tuning()\n",
        "\n",
        "    # Random search\n",
        "    # random_search_tuning() # Removed as requested\n",
        "\n",
        "    # Learning curves\n",
        "    learning_curve_analysis()\n",
        "\n",
        "    # Model comparison\n",
        "    compare_models()\n",
        "\n",
        "    # Bias-variance tradeoff\n",
        "    bias_variance_tradeoff()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Lab 14 Complete!\")\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QHySeDt0D8h",
        "outputId": "f8524d29-16e1-4fe7-b2ca-890ed0168854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Lab 14: Model Evaluation and Tuning\n",
            "==================================================\n",
            "==================================================\n",
            "Classification Metrics\n",
            "==================================================\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy:  0.9708\n",
            "Precision: 0.9640\n",
            "Recall:    0.9907\n",
            "F1-Score:  0.9772\n",
            "ROC-AUC:   0.9969\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.94      0.96        63\n",
            "      benign       0.96      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.96      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n",
            "\n",
            "Confusion matrix saved as 'lab14_confusion_matrix.png'\n",
            "\n",
            "==================================================\n",
            "ROC Curve Analysis\n",
            "==================================================\n",
            "\n",
            "Area Under ROC Curve: 0.9969\n",
            "\n",
            "ROC curve saved as 'lab14_roc_curve.png'\n",
            "\n",
            "==================================================\n",
            "Cross-Validation Evaluation\n",
            "==================================================\n",
            "\n",
            "5-Fold Cross-Validation Results:\n",
            "Scores: [0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n",
            "Mean Accuracy: 0.9667\n",
            "Standard Deviation: 0.0211\n",
            "95% Confidence Interval: [0.9253, 1.0080]\n",
            "\n",
            "Cross-validation plot saved as 'lab14_cross_validation.png'\n",
            "\n",
            "==================================================\n",
            "Grid Search Hyperparameter Tuning\n",
            "==================================================\n",
            "\n",
            "Parameter Grid:\n",
            "  n_estimators: [50, 100, 200]\n",
            "  max_depth: [3, 5, 7, None]\n",
            "  min_samples_split: [2, 5, 10]\n",
            "\n",
            "Performing Grid Search...\n",
            "\n",
            "Best Parameters:\n",
            "  max_depth: 3\n",
            "  min_samples_split: 2\n",
            "  n_estimators: 200\n",
            "\n",
            "Best Cross-Validation Score: 0.9429\n",
            "Test Set Score: 1.0000\n",
            "\n",
            "Grid search results saved as 'lab14_grid_search.png'\n",
            "\n",
            "==================================================\n",
            "Learning Curve Analysis\n",
            "==================================================\n",
            "\n",
            "Learning Curve Summary:\n",
            "Final Training Score: 1.0000 (+/- 0.0000)\n",
            "Final Validation Score: 0.9173 (+/- 0.0242)\n",
            "\n",
            "Learning curve saved as 'lab14_learning_curve.png'\n",
            "\n",
            "==================================================\n",
            "Model Comparison\n",
            "==================================================\n",
            "\n",
            "Evaluating models with 5-fold cross-validation:\n",
            "\n",
            "Random Forest:\n",
            "  Mean Accuracy: 0.9667\n",
            "  Std Dev: 0.0211\n",
            "\n",
            "Decision Tree:\n",
            "  Mean Accuracy: 0.9533\n",
            "  Std Dev: 0.0340\n",
            "\n",
            "SVM:\n",
            "  Mean Accuracy: 0.9667\n",
            "  Std Dev: 0.0211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1892535609.py:51: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "  ax2.boxplot(all_scores, labels=model_names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model comparison plot saved as 'lab14_model_comparison.png'\n",
            "\n",
            "==================================================\n",
            "Bias-Variance Tradeoff\n",
            "==================================================\n",
            "\n",
            "Testing different model complexities:\n",
            "Max Depth 5: Train=0.9950, Test=0.9532\n",
            "Max Depth 10: Train=1.0000, Test=0.9415\n",
            "Max Depth 15: Train=1.0000, Test=0.9415\n",
            "Max Depth 20: Train=1.0000, Test=0.9415\n",
            "\n",
            "Bias-variance plot saved as 'lab14_bias_variance.png'\n",
            "\n",
            "==================================================\n",
            "Lab 14 Complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}